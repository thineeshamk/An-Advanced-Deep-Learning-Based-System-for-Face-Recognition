# -*- coding: utf-8 -*-
"""MobileNetV2 + Fine-Tuned (Model 3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18jg8TMio0G86GrdVVeZXIHIV_US02GLp
"""

import os
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers, models

# Define dataset path
dataset_path = "/content/drive/MyDrive/Facial image recodnition"

# Get all image paths and labels
image_paths = []
labels = []
class_names = sorted(os.listdir(dataset_path))

for class_idx, class_name in enumerate(class_names):
    class_dir = os.path.join(dataset_path, class_name)
    for img_name in os.listdir(class_dir):
        image_paths.append(os.path.join(class_dir, img_name))
        labels.append(class_idx)

# Convert to NumPy arrays
image_paths = np.array(image_paths)
labels = np.array(labels)

# Stratified split (80% train, 20% validation)
train_paths, val_paths, train_labels, val_labels = train_test_split(
    image_paths, labels, test_size=0.2, stratify=labels, random_state=123
)

# Data Augmentation
data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
    layers.RandomContrast(0.1)
])

# Function to load and preprocess images
def load_image(image_path, label, augment=False):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, (224, 224))
    image = image / 255.0
    if augment:
        image = data_augmentation(image)
    return image, label

# Create TensorFlow datasets
train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))
train_ds = train_ds.map(lambda x, y: load_image(x, y, augment=True)).batch(8).shuffle(80).prefetch(tf.data.AUTOTUNE)

val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))
val_ds = val_ds.map(load_image).batch(8).prefetch(tf.data.AUTOTUNE)

# Import necessary modules
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback

# Build the model
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

inputs = Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)
x = Conv2D(64, (3,3), activation='relu', padding='same')(x)
x = MaxPooling2D((2,2))(x)
x = Conv2D(32, (3,3), activation='relu', padding='same')(x)
x = MaxPooling2D((2,2))(x)
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(10, activation='softmax')(x)

model = Model(inputs, outputs)

# Compile with initial learning rate
model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Custom Callback to Unfreeze Last 40 Layers
class UnfreezeCallback(Callback):
    def __init__(self, unfreeze_at_epoch=25, unfreeze_layers=40):
        super().__init__()
        self.unfreeze_at_epoch = unfreeze_at_epoch
        self.unfreeze_layers = unfreeze_layers

    def on_epoch_begin(self, epoch, logs=None):
        if epoch == self.unfreeze_at_epoch:
            print(f"\nðŸ”“ Unfreezing last {self.unfreeze_layers} layers of the base model...")
            for layer in base_model.layers[:-self.unfreeze_layers]:
                layer.trainable = False
            for layer in base_model.layers[-self.unfreeze_layers:]:
                layer.trainable = True

            # Recompile with a lower learning rate
            self.model.compile(optimizer=Adam(learning_rate=1e-5),
                               loss='sparse_categorical_crossentropy',
                               metrics=['accuracy'])

# First training phase (frozen base model)
history = model.fit(train_ds,
                    validation_data=val_ds,
                    epochs=25,
                    callbacks=[UnfreezeCallback()])

# Optional: Explicitly re-unfreeze last 40 layers again and fine-tune (alternative approach)
base_model.trainable = True
for layer in base_model.layers[:-40]:
    layer.trainable = False

# Recompile for fine-tuning
model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Continue training (fine-tuning phase)
history_fine = model.fit(train_ds,
                         validation_data=val_ds,
                         epochs=50,
                         initial_epoch=25)

# Save the trained model ---
model.save("face_recognition_model.h5")
print("Model saved as face_recognition_model.h5")

import matplotlib.pyplot as plt

# Combine histories
def combine_histories(h1, h2):
    acc = h1.history['accuracy'] + h2.history['accuracy']
    val_acc = h1.history['val_accuracy'] + h2.history['val_accuracy']
    loss = h1.history['loss'] + h2.history['loss']
    val_loss = h1.history['val_loss'] + h2.history['val_loss']
    return acc, val_acc, loss, val_loss

acc, val_acc, loss, val_loss = combine_histories(history, history_fine)

epochs_range = range(1, len(acc) + 1)

# Plot Accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.axvline(x=25, color='gray', linestyle='--', label='Fine-tuning starts')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.axvline(x=25, color='gray', linestyle='--')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.tight_layout()
plt.show()